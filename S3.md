# S3

* Simple Storage Service.
* 11 9's (99.999999999) durability.
* Designed for 99.5% to 99.99% availability (although SLAs are lower)

### Limits

* Each object, maximum of 5TB.
* Single PUT is limited to 5GB.
* Use muiltipart upload for objects > 100 MB.

### Bucket Types

* General Purpose
    * default, recommended for majority of use cases
    * flat storage structure
    * objects stored across at minimum 3 AZs in then given region
      
* Directory
    * for performance-critical applications that require consistent single-digit millisecond latency
    * organize data hierarchically into directories
    * can elastically scale performance to support hundreds of thousands of transactions per second (TPS)
    * support only the S3 Express One Zone storage class and a limited set of Amazon S3 features
      
### Permissions

* S3 Object Ownership
    * ACLs disabled (recommended)
        * Bucket owner owns all object in the bucket
        * Access managed exclusibvely by policies
    * ACLs enabled -
       * Bucket owner preferred
          * The bucket owner owns new objects that other accounts write to the bucket with the bucket-owner-full-control canned ACL
       * Object writer
           * The AWS account that uploads an object owns the object, and can grant other users access to it through ACLs
 
* Block Public Access settings for this bucket
    * Block all public access (default)
    * AWS recommends that you turn on block all public access, unless public access is required for specific and verified use cases such as static website hosting.
 
### Storage Classes

* S3 Standard
    * General purpose store for frequently accessed data.
    * Data replicateed across at least 3 AZs
    * 99.999999999% durability; 99.99% availability
    * Low latency and high throughput
    * Charge for storage
* S3 Standard-IA (Infrequent Access)
    * Long-lived less frequently accessed data.
    * Data replicateed across at least 3 AZs
    * 99.999999999% durability; 99.9% availability
    * Lower cost for storage, but charge for retrieval
* S3 One Zone-IA
    * Long-lived less frequently accessed data.
    * Data stored in 1 AZ
    * 99.999999999% durability; 99.5% availability
    * 20% Lower cost for storage than S3 Standard-IA, but charge for retrieval
* S3 Intelligent-Tiering
    * Data with unknown or changing access patterns.
    * Monitors access patterns.
    * Moves objects that have not been accessed in 30 days to IA
    * Cost optimization
* S3 Glacier
    * Long-term archive
* S3 Glacier Deep Archive
    * Long-term archive
* S3 Outposts
    * Store S3 data on-premise.
* S3 Intelligent-Tiering
    * Data with unknown or changing access patterns.

![](./images/aws-s3.png)

### Consistency Model

* Strong read-after-write consistency (for PUTs and DELETEs)
* Eventual consistency (for bucket configurations, including bucket DELETEs and enabling versioning - wait 15 mins).

### Pricing Model

* Storage costs
    * can vary significantly by storage class.
* Request and Data Retrieval costs (Data retrieval from IA and  Glacier archives)
* Data transfer costs, except for:
    * Data transferred in from the internet.
    * Data transferred between S3 buckets in the same AWS Region. 
    * Data transferred from an Amazon S3 bucket to any AWS service(s) within the same AWS Region as the S3 bucket (including to a different account in the same AWS Region).
    * Data transferred out to Amazon CloudFront (CloudFront).

### S3 Replication

* Asynchronous replication across buckets
* Same-Region replication and Cross-Region replication 
    * automatically replicates new objects.
* Batch replication
    * replicates exiting objects 

### S3 Events

### S3 Logging


### Intelligent-Tiering Archive configuration

### S3 Glacier

* Files are stored as archives.
    * An archive can be up to 40TB.
* Archives are grouped in vaults.
* Event notifications can be enabled for the following job completion events:
    * Archive Retrieval Job Complete.
    * Vault Inventory Retrieval Job Complete.

### S3 Access Points

* Access points are named network endpoints that are attached to buckets that you can use to perform S3 object operations.
* Why?
    * Shared data sets
        * Decompose one large bucket policy into separate, discrete access point policies for each application that needs to access the shared data set.
    * Copy data securely
        * Between same-region access points.
    * Limit access to specific account IDs and VPCs
    * Provide a unique name within a region.

### S3 Object Lambda Access Points

* Add lambda functions to S3 GET, HEAD, and LIST requests to modify and process data as it is returned to an application
* ![](https://docs.aws.amazon.com/images/AmazonS3/latest/userguide/images/ol-example-image-global.png)

### Encryption
* Disabled by default
* SSE-S3
    * S3 managed keys
    * Reduce the cost of SSE by up to 99% by decresing request traffic from S3 to KMS.
* SSE-KMS
    * KMS managed keys
    * Choice between AWSKMS managed key (aws/s3) of a custiomer managed key.
    * Why use a customer managed key:
        *  You want to grant cross-account access to your S3 objects.
        *  You want to create, rotate, diable or grant access policies for the key.     
* To encrypt existing Amazon S3 objects use tghe Batch Operations Copy operation.

### Access Analyzer for S3

Alerts you to S3 buckets that are configured to allow access to anyone on the internet or other AWS accounts, including accounts outside of your organization.
